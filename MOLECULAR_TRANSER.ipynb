{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yamenfargaly-maker/TRANSFORMER/blob/main/MOLECULAR_TRANSER.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ue-Vl5PHV8O9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ffd0dd-17cf-495a-b801-557c892ecf1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n",
            "Requirement already satisfied: rdkit in /usr/local/lib/python3.12/dist-packages (2025.3.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Collecting git+https://github.com/connorcoley/rdchiral.git\n",
            "  Cloning https://github.com/connorcoley/rdchiral.git to /tmp/pip-req-build-1gs1mxwz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/connorcoley/rdchiral.git /tmp/pip-req-build-1gs1mxwz\n",
            "  Resolved https://github.com/connorcoley/rdchiral.git to commit da174b921b921f6547e46f32812b5d4af937cc94\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "## Cell 1 — Code — Install dependencies\n",
        "\n",
        "# 2. Downgrade numpy before installing rdkit\n",
        "!pip install \"numpy<2.0\"\n",
        "\n",
        "# 3. Install rdkit\n",
        "!pip install rdkit\n",
        "\n",
        "# 4. Install rdchiral properly\n",
        "!pip install git+https://github.com/connorcoley/rdchiral.git\n",
        "\n",
        "# 5. Other dependencies\n",
        "!pip install torch torchvision torchaudio pandas tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 2 — Code — Create folders / scaffolding\n",
        "\n",
        "import os\n",
        "\n",
        "os.makedirs(\"retroB/data\", exist_ok=True)\n",
        "os.makedirs(\"retroB/mt\", exist_ok=True)\n",
        "os.makedirs(\"retroB/runs\", exist_ok=True)\n",
        "os.makedirs(\"retroB/out\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "XwmobtuBWPVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 3 — Code — Helper to write modules\n",
        "\n",
        "def write_script(path, code):\n",
        "    with open(path, 'w') as f:\n",
        "        f.write(code)"
      ],
      "metadata": {
        "id": "5EUPaB9fWPsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 4 - Code - Cleans, canonicalizes, and randomly splits a CSV of chemical reactions into 80% training, 10% validation, and 10% test sets, saved as text files\n",
        "## Takes in \"reaction_smiles\" (reactant>>reagents>>products) as column input and outputs \"processed\" train, validation, and test text files with reaction id, product, and reactant for each, separated by tab-separated\n",
        "## Saves in prepare_upsto.py to be run later on\n",
        "\n",
        "prep_code = \"\"\"\n",
        "import pandas as pd, re, argparse, random, os\n",
        "from rdkit import Chem\n",
        "\n",
        "def canon(smi: str):\n",
        "    if pd.isna(smi): return None\n",
        "    mol = Chem.MolFromSmiles(str(smi))\n",
        "    if not mol: return None\n",
        "    Chem.SanitizeMol(mol)\n",
        "    return Chem.MolToSmiles(mol, canonical=True)\n",
        "\n",
        "def unmap(rx: str):\n",
        "    # remove atom-map labels like :1 :2 etc\n",
        "    return re.sub(r':\\\\d+', '', str(rx))\n",
        "\n",
        "def split_df(df: pd.DataFrame, seed=1337):\n",
        "    ids = list(df.index)\n",
        "    random.Random(seed).shuffle(ids)\n",
        "    n = len(ids); n_tr, n_va = int(0.8*n), int(0.1*n)\n",
        "    tr = df.loc[ids[:n_tr]]; va = df.loc[ids[n_tr:n_tr+n_va]]; te = df.loc[ids[n_tr+n_va:]]\n",
        "    return tr, va, te\n",
        "\n",
        "def to_lines(df: pd.DataFrame, out_path: str):\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    with open(out_path, \"w\") as f:\n",
        "        for rid, r in df.iterrows():\n",
        "            p, q = r[\"product_canon\"], r[\"reactants_canon\"]\n",
        "            if pd.isna(p) or pd.isna(q):\n",
        "                continue\n",
        "            f.write(f\"{rid}\\\\t{p}\\\\t{q}\\\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--csv\", required=True, help=\"CSV with column 'reactants>reagents>production' (reactants>>product only)\")\n",
        "    ap.add_argument(\"--outdir\", default=\"data/processed\")\n",
        "    args = ap.parse_args()\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "\n",
        "    df = pd.read_csv(args.csv)\n",
        "\n",
        "    # get the reactions column\n",
        "    colname = \"reactants>reagents>production\"\n",
        "    if colname not in df.columns:\n",
        "        raise ValueError(f\"Expected column '{colname}' not found\")\n",
        "\n",
        "    rx = df[colname].astype(str).apply(unmap)\n",
        "\n",
        "    # split into reactants and product\n",
        "    df[\"reactants\"] = rx.str.split(\">>\").str[0]\n",
        "    df[\"product\"]   = rx.str.split(\">>\").str[1]\n",
        "\n",
        "    # canonicalize both sides\n",
        "    df[\"reactants_canon\"] = df[\"reactants\"].apply(canon)\n",
        "    df[\"product_canon\"]   = df[\"product\"].apply(canon)\n",
        "    df = df.dropna(subset=[\"reactants_canon\",\"product_canon\"]).reset_index(drop=True)\n",
        "\n",
        "    # split and save\n",
        "    tr, va, te = split_df(df)\n",
        "    to_lines(tr, f\"{args.outdir}/train.txt\")\n",
        "    to_lines(va, f\"{args.outdir}/valid.txt\")\n",
        "    to_lines(te, f\"{args.outdir}/test.txt\")\n",
        "\"\"\"\n",
        "write_script(\"retroB/data/prepare_uspto.py\", prep_code)"
      ],
      "metadata": {
        "id": "LLDfDAIVWP5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 5\n",
        "## Tokenzes SMILES strings into characters, builds a vocab of allowed characters, which are then converted into integer IDs\n",
        "## PAD - Makes all sequences in a batch the same length\n",
        "## BOS - Marks where decoding starts\n",
        "## EOS - Marks where decoding ends.\n",
        "## Saves in tokenizer.py\n",
        "\n",
        "tok_code = \"\"\"\n",
        "PAD, BOS, EOS = \"<pad>\", \"<bos>\", \"<eos>\"\n",
        "\n",
        "def tok_smiles(s: str):\n",
        "    return list(s.strip())\n",
        "\n",
        "def build_vocab(paths, min_freq=1):\n",
        "    from collections import Counter\n",
        "    cnt = Counter()\n",
        "    for p in paths:\n",
        "        for line in open(p, \"r\"):\n",
        "            _, prod, react = line.rstrip(\"\\\\n\").split(\"\\\\t\")\n",
        "            cnt.update(tok_smiles(prod)); cnt.update(tok_smiles(react))\n",
        "    itos = [PAD, BOS, EOS] + sorted([t for t,c in cnt.items() if c >= min_freq])\n",
        "    stoi = {t:i for i,t in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "def encode(tokens, stoi):\n",
        "    return [stoi[t] for t in tokens]\n",
        "\n",
        "def decode(ids, itos):\n",
        "    # join chars, skipping specials\n",
        "    specials = {PAD, BOS, EOS}\n",
        "    toks = [itos[i] for i in ids if itos[i] not in specials]\n",
        "    return \"\".join(toks)\n",
        "\"\"\"\n",
        "write_script(\"retroB/mt/tokenizer.py\", tok_code)"
      ],
      "metadata": {
        "id": "gJSIb-sSWQFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 6\n",
        "## Takes in tokenized SMILES strings and learns to \"translate\" products --> reactants\n",
        "## Adds positional information so the model understands the SMILES structures\n",
        "## Uses a custmoo learning rate schedule called Noam Decay: INcreasings the learning rate for the first few thousand steps and then decays over time to stablize the model's learning capabilities\n",
        "\n",
        "\n",
        "model_code = \"\"\"\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=4096):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))\n",
        "        pe[:,0::2] = torch.sin(pos*div); pe[:,1::2] = torch.cos(pos*div)\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))  # [1, L, D]\n",
        "    def forward(self, x):  # x: [B,L,D]\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "class MT(nn.Module):\n",
        "    def __init__(self, vocab, d_model=512, nhead=8, num_layers=6, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        V = len(vocab)\n",
        "        self.src_emb = nn.Embedding(V, d_model)\n",
        "        self.tgt_emb = nn.Embedding(V, d_model)\n",
        "        self.pos     = PositionalEncoding(d_model)\n",
        "        self.tf = nn.Transformer(\n",
        "            d_model=d_model, nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=d_ff, dropout=dropout, batch_first=True)\n",
        "        self.proj = nn.Linear(d_model, V)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.pos(self.dropout(self.src_emb(src)))\n",
        "        tgt = self.pos(self.dropout(self.tgt_emb(tgt)))\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "        out = self.tf(src, tgt, tgt_mask=tgt_mask)\n",
        "        return self.proj(out)  # [B,L,V]\n",
        "\n",
        "def noam_lr(opt, d_model, warmup=8000):\n",
        "    step = 0\n",
        "    def _rate():\n",
        "        nonlocal step\n",
        "        step += 1\n",
        "        return (d_model**-0.5) * min(step**-0.5, step*(warmup**-1.5))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda=lambda _: _rate())\n",
        "\"\"\"\n",
        "write_script(\"retroB/mt/model.py\", model_code)"
      ],
      "metadata": {
        "id": "yne6nMQaWQSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Cell 7\n",
        "##Baseline with standard cross-entropy\n",
        "train_code = \"\"\"\n",
        "import argparse, torch, os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tokenizer import build_vocab, tok_smiles, encode, decode, PAD, BOS, EOS\n",
        "from model import MT, noam_lr\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "RDLogger.DisableLog(\"rdApp.*\")\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class RxDataset(Dataset):\n",
        "    def __init__(self, path):\n",
        "        # Each line: reaction_id \\\\t product \\\\t reactants\n",
        "        self.rows = [l.rstrip(\"\\\\n\").split(\"\\\\t\") for l in open(path)]\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self, i):\n",
        "        rid, p, r = self.rows[i]\n",
        "        return rid, p, r\n",
        "\n",
        "def collate(batch, stoi, max_len=256):\n",
        "    rids, src, tgt = zip(*batch)\n",
        "    src = [[BOS]+tok_smiles(s)+[EOS] for s in src]\n",
        "    tgt = [[BOS]+tok_smiles(s)+[EOS] for s in tgt]\n",
        "    src_ids = [encode(s, stoi)[:max_len] for s in src]\n",
        "    tgt_ids = [encode(t, stoi)[:max_len] for t in tgt]\n",
        "    pad_id = stoi[PAD]\n",
        "    Ls = max(map(len, src_ids)); Lt = max(map(len, tgt_ids))\n",
        "    pad = lambda seq, L: seq + [pad_id]*(L-len(seq))\n",
        "    src_t = torch.tensor([pad(s,Ls) for s in src_ids]).long()\n",
        "    tgt_in = torch.tensor([pad(t[:-1],Lt-1) for t in tgt_ids]).long()\n",
        "    tgt_y  = torch.tensor([pad(t[1:], Lt-1) for t in tgt_ids]).long()\n",
        "    return rids, src_t, tgt_in, tgt_y\n",
        "\n",
        "# -------------------------\n",
        "# Loss\n",
        "# -------------------------\n",
        "def ce_loss(logits, target, ignore_index=0):\n",
        "    return F.cross_entropy(\n",
        "        logits.transpose(1,2),\n",
        "        target,\n",
        "        ignore_index=ignore_index,\n",
        "        reduction=\"mean\"\n",
        "    )\n",
        "\n",
        "# -------------------------\n",
        "# Beam Search Decoder\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def beam_search(model, src, stoi, itos, beam=10, max_len=256):\n",
        "    model.eval()\n",
        "    pad_id, bos_id, eos_id = stoi[PAD], stoi[BOS], stoi[EOS]\n",
        "    src = src.to(DEVICE)\n",
        "\n",
        "    beams = [([bos_id], 0.0)]  # sequence, score\n",
        "    finished = []\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        new_beams = []\n",
        "        for seq, score in beams:\n",
        "            if seq[-1] == eos_id:\n",
        "                finished.append((seq, score))\n",
        "                continue\n",
        "            tgt = torch.tensor(seq).unsqueeze(0).to(DEVICE)\n",
        "            logits = model(src.unsqueeze(0), tgt)[:,-1]  # last step\n",
        "            probs = F.log_softmax(logits, dim=-1).squeeze(0)\n",
        "            topk = torch.topk(probs, beam)\n",
        "            for idx, s in zip(topk.indices.tolist(), topk.values.tolist()):\n",
        "                new_beams.append((seq+[idx], score+s))\n",
        "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam]\n",
        "\n",
        "    finished.extend(beams)\n",
        "    finished = sorted(finished, key=lambda x: x[1], reverse=True)\n",
        "    decoded = [decode(seq, itos) for seq, _ in finished]\n",
        "    return decoded[:beam]\n",
        "\n",
        "\n",
        "def canonicalize(smi):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol:\n",
        "            return Chem.MolToSmiles(mol, canonical=True)\n",
        "    except:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, stoi, itos, beam=10):\n",
        "    model.eval()\n",
        "    tot_loss, n = 0.0, 0\n",
        "    top1, top5, top10 = 0, 0, 0\n",
        "\n",
        "    for _, src, tgt_in, tgt_y in loader:\n",
        "        src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "\n",
        "        # Loss\n",
        "        logits = model(src, tgt_in)\n",
        "        tot_loss += ce_loss(logits, tgt_y, ignore_index=stoi[PAD]).item()\n",
        "        n += 1\n",
        "\n",
        "        # Decode & compare\n",
        "        for i in range(src.size(0)):\n",
        "            beams = beam_search(model, src[i], stoi, itos, beam=beam)\n",
        "\n",
        "            # decode target\n",
        "            target = decode(tgt_y[i].tolist(), itos)\n",
        "\n",
        "            # canonicalize safely\n",
        "            can_target = canonicalize(target)\n",
        "            can_beams  = [canonicalize(b) for b in beams if canonicalize(b) is not None]\n",
        "\n",
        "            if can_target and can_target in can_beams[:1]:\n",
        "                top1 += 1\n",
        "            if can_target and can_target in can_beams[:5]:\n",
        "                top5 += 1\n",
        "            if can_target and can_target in can_beams[:10]:\n",
        "                top10 += 1\n",
        "\n",
        "    return (\n",
        "        tot_loss/max(1,n),\n",
        "        top1/len(loader.dataset),\n",
        "        top5/len(loader.dataset),\n",
        "        top10/len(loader.dataset)\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main(args):\n",
        "    stoi, itos = build_vocab([args.train, args.valid])\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    torch.save({\"stoi\":stoi, \"itos\":itos}, os.path.join(args.outdir,\"vocab.pt\"))\n",
        "\n",
        "    tr = RxDataset(args.train); va = RxDataset(args.valid)\n",
        "    C = lambda b: collate(b, stoi)\n",
        "    dl_tr = DataLoader(tr, batch_size=args.bsz, shuffle=True, collate_fn=C)\n",
        "    dl_va = DataLoader(va, batch_size=1, shuffle=False, collate_fn=C)  # beam search needs batch=1\n",
        "\n",
        "    model = MT(stoi).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1.0, betas=(0.9,0.98), weight_decay=0.01)\n",
        "    sch = noam_lr(opt, d_model=512, warmup=8000)\n",
        "\n",
        "    best, bad = 1e9, 0\n",
        "    patience = args.patience\n",
        "    for epoch in range(args.max_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        for _, src, tgt_in, tgt_y in dl_tr:\n",
        "            src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = ce_loss(logits, tgt_y, ignore_index=stoi[PAD])\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sch.step()\n",
        "\n",
        "        # Validation\n",
        "        vloss, acc1, acc5, acc10 = validate(model, dl_va, stoi, itos)\n",
        "        print(f\"epoch {epoch} | val_loss {vloss:.4f} | top1 {acc1:.3f} | top5 {acc5:.3f} | top10 {acc10:.3f}\")\n",
        "\n",
        "        if vloss < best:\n",
        "            best, bad = vloss, 0\n",
        "            torch.save({\"model\":model.state_dict(),\"stoi\":stoi,\"itos\":itos},\n",
        "                       os.path.join(args.outdir,\"best.pt\"))\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience: break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--train\", default=\"data/processed/train.txt\")\n",
        "    ap.add_argument(\"--valid\", default=\"data/processed/valid.txt\")\n",
        "    ap.add_argument(\"--outdir\", default=\"runs/mt_standardce\")\n",
        "    ap.add_argument(\"--bsz\", type=int, default=64)\n",
        "    ap.add_argument(\"--max_epochs\", type=int, default=8)\n",
        "    ap.add_argument(\"--patience\", type=int, default=5, help=\"Early stopping patience (epochs without improvement)\")\n",
        "\n",
        "    args = ap.parse_args(); main(args)\n",
        "\"\"\"\n",
        "write_script(\"retroB/mt/train.py\", train_code)"
      ],
      "metadata": {
        "id": "NXlyMwc0Wcgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Baseline + Label Smoothing Cross-Entropy\n",
        "train_code = \"\"\"\n",
        "import argparse, torch, os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tokenizer import build_vocab, tok_smiles, encode, PAD, BOS, EOS\n",
        "from model import MT, noam_lr\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RxDataset(Dataset):\n",
        "    def __init__(self, path): self.rows = [l.rstrip(\"\\\\n\").split(\"\\\\t\") for l in open(path)]\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self,i): rid, p, r = self.rows[i]; return rid, p, r\n",
        "\n",
        "def collate(batch, stoi, max_len=256):\n",
        "    rids, src, tgt = zip(*batch)\n",
        "    src = [[BOS]+tok_smiles(s)+[EOS] for s in src]\n",
        "    tgt = [[BOS]+tok_smiles(s)+[EOS] for s in tgt]\n",
        "    src_ids = [encode(s, stoi)[:max_len] for s in src]\n",
        "    tgt_ids = [encode(t, stoi)[:max_len] for t in tgt]\n",
        "    pad_id = stoi[PAD]\n",
        "    Ls = max(map(len, src_ids)); Lt = max(map(len, tgt_ids))\n",
        "    pad = lambda seq, L: seq + [pad_id]*(L-len(seq))\n",
        "    src_t = torch.tensor([pad(s,Ls) for s in src_ids]).long()\n",
        "    tgt_in = torch.tensor([pad(t[:-1],Lt-1) for t in tgt_ids]).long()\n",
        "    tgt_y  = torch.tensor([pad(t[1:], Lt-1) for t in tgt_ids]).long()\n",
        "    return rids, src_t, tgt_in, tgt_y\n",
        "\n",
        "def label_smoothing_loss(logits, target, eps=0.1, ignore_index=0, reduction=\"none\"):\n",
        "    n_class = logits.size(-1)\n",
        "    logp = F.log_softmax(logits, -1)\n",
        "    with torch.no_grad():\n",
        "        true = torch.zeros_like(logp)\n",
        "        true.fill_(eps/(n_class-1))\n",
        "        true.scatter_(2, target.unsqueeze(-1), 1-eps)\n",
        "        true.masked_fill_(target.eq(ignore_index).unsqueeze(-1), 0)\n",
        "    loss = -(true*logp).sum(-1)\n",
        "    mask = (~target.eq(ignore_index)).float()\n",
        "    loss = loss * mask\n",
        "    if reduction == \"none\":\n",
        "        return loss.sum(-1) / mask.sum(-1)  # per-example loss\n",
        "    else:\n",
        "        return loss.sum()/mask.sum()\n",
        "\n",
        "# -------------------------\n",
        "# Load precomputed UQ weights\n",
        "# -------------------------\n",
        "def load_uq_weights(path):\n",
        "    df = pd.read_csv(path)\n",
        "    top1 = df[df[\"rank\"] == 1][[\"reaction_id\", \"UQ_score\"]]\n",
        "    # normalize: low UQ = high weight\n",
        "    uq = 1.0 - (top1[\"UQ_score\"] - top1[\"UQ_score\"].min()) / (top1[\"UQ_score\"].max() - top1[\"UQ_score\"].min() + 1e-8)\n",
        "    weights = dict(zip(top1[\"reaction_id\"], uq))\n",
        "    return weights\n",
        "\n",
        "def main(args):\n",
        "    stoi, itos = build_vocab([args.train, args.valid])\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    torch.save({\"stoi\":stoi, \"itos\":itos}, os.path.join(args.outdir, \"vocab.pt\"))\n",
        "\n",
        "    tr = RxDataset(args.train); va = RxDataset(args.valid)\n",
        "    C = lambda b: collate(b, stoi)\n",
        "    dl_tr = DataLoader(tr, batch_size=args.bsz, shuffle=True,  collate_fn=C)\n",
        "    dl_va = DataLoader(va, batch_size=args.bsz, shuffle=False, collate_fn=C)\n",
        "\n",
        "    # load UQ weights if provided\n",
        "    uq_weights = load_uq_weights(args.uq_csv) if args.uq_csv else {}\n",
        "\n",
        "    model = MT(stoi).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1.0, betas=(0.9,0.98), weight_decay=0.01)\n",
        "    sch = noam_lr(opt, d_model=512, warmup=8000)\n",
        "\n",
        "    best, bad = 1e9, 0\n",
        "    patience = args.patience\n",
        "    for epoch in range(args.max_epochs):   # <-- capped by CLI arg\n",
        "        model.train()\n",
        "        for rids, src, tgt_in, tgt_y in dl_tr:\n",
        "            src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "            logits = model(src, tgt_in)\n",
        "            per_ex_loss = label_smoothing_loss(logits, tgt_y, ignore_index=stoi[PAD], reduction=\"none\")\n",
        "\n",
        "            # apply UQ weighting\n",
        "            if uq_weights:\n",
        "                w = torch.tensor([uq_weights.get(rid, 1.0) for rid in rids], device=DEVICE)\n",
        "                loss = (per_ex_loss * w).mean()\n",
        "            else:\n",
        "                loss = per_ex_loss.mean()\n",
        "\n",
        "            opt.zero_grad(); loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            opt.step(); sch.step()\n",
        "\n",
        "        # validation\n",
        "        model.eval(); vloss, cnt = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for _, src, tgt_in, tgt_y in dl_va:\n",
        "                src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "                logits = model(src, tgt_in)\n",
        "                vloss += label_smoothing_loss(logits, tgt_y, ignore_index=stoi[PAD]).mean().item(); cnt += 1\n",
        "        vloss /= max(cnt,1)\n",
        "        print(f\"epoch {epoch} | val_loss {vloss:.4f}\")\n",
        "\n",
        "        if vloss < best:\n",
        "            best, bad = vloss, 0\n",
        "            torch.save({\"model\":model.state_dict(),\"stoi\":stoi,\"itos\":itos},\n",
        "                       os.path.join(args.outdir,\"best.pt\"))\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience: break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--train\",  default=\"data/processed/train.txt\")\n",
        "    ap.add_argument(\"--valid\",  default=\"data/processed/valid.txt\")\n",
        "    ap.add_argument(\"--outdir\", default=\"runs/mt\")\n",
        "    ap.add_argument(\"--bsz\", type=int, default=64)\n",
        "    ap.add_argument(\"--uq_csv\", default=\"\", help=\"CSV of precomputed UQ scores for training set\")\n",
        "    ap.add_argument(\"--max_epochs\", type=int, default=8, help=\"maximum number of epochs to train\")  # <-- added arg\n",
        "    args = ap.parse_args(); main(args)\n",
        "\"\"\"\n",
        "\n",
        "write_script(\"retroB/mt/train.py\", train_code)"
      ],
      "metadata": {
        "id": "RODF2RbYWcv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Baseline + UQ + Standard Cross Entropy\n",
        "train_code = \"\"\"\n",
        "import argparse, os, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tokenizer import build_vocab, tok_smiles, encode, PAD, BOS, EOS\n",
        "from model import MT, noam_lr\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# Dataset / Collate\n",
        "# -------------------------\n",
        "class RxDataset(Dataset):\n",
        "    def __init__(self, path): self.rows = [l.rstrip(\"\\\\n\").split(\"\\\\t\") for l in open(path)]\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self,i): rid, p, r = self.rows[i]; return rid, p, r\n",
        "\n",
        "def collate(batch, stoi, max_len=256):\n",
        "    rids, src, tgt = zip(*batch)\n",
        "    src = [[BOS] + tok_smiles(s) + [EOS] for s in src]\n",
        "    tgt = [[BOS] + tok_smiles(s) + [EOS] for s in tgt]\n",
        "    src_ids = [encode(s, stoi)[:max_len] for s in src]\n",
        "    tgt_ids = [encode(t, stoi)[:max_len] for t in tgt]\n",
        "    pad_id = stoi[PAD]\n",
        "    Ls = max(map(len, src_ids)); Lt = max(map(len, tgt_ids))\n",
        "    pad = lambda seq, L: seq + [pad_id] * (L - len(seq))\n",
        "    src_t = torch.tensor([pad(s, Ls) for s in src_ids]).long()\n",
        "    tgt_in = torch.tensor([pad(t[:-1], Lt - 1) for t in tgt_ids]).long()\n",
        "    tgt_y  = torch.tensor([pad(t[1:],  Lt - 1) for t in tgt_ids]).long()\n",
        "    return rids, src_t, tgt_in, tgt_y\n",
        "\n",
        "# -------------------------\n",
        "# Pure cross-entropy loss\n",
        "# -------------------------\n",
        "def ce_loss(logits, target, ignore_index=0, reduction=\"none\"):\n",
        "    loss = F.cross_entropy(\n",
        "        logits.transpose(1, 2),  # [B, V, T]\n",
        "        target,\n",
        "        ignore_index=ignore_index,\n",
        "        reduction=\"none\"\n",
        "    )  # [B, T]\n",
        "    if reduction == \"none\":\n",
        "        mask = (~target.eq(ignore_index)).float()\n",
        "        loss = (loss * mask).sum(-1) / mask.sum(-1).clamp_min(1.0)\n",
        "        return loss\n",
        "    else:\n",
        "        return loss.mean()\n",
        "\n",
        "# -------------------------\n",
        "# Uncertainty helpers\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def seq_entropy(logits):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    ent = -(probs * log_probs).sum(-1)     # [B, T]\n",
        "    return ent.mean(-1)                     # [B]\n",
        "\n",
        "@torch.no_grad()\n",
        "def mc_dropout_uq(model, src, tgt_in, n_samples=6):\n",
        "    model.train()  # enable dropout\n",
        "    preds = []\n",
        "    for _ in range(max(1, n_samples)):\n",
        "        logits = model(src, tgt_in)\n",
        "        preds.append(F.softmax(logits, dim=-1).detach())\n",
        "    stack = torch.stack(preds, dim=0)  # [S, B, T, V]\n",
        "    var = stack.var(dim=0)             # [B, T, V]\n",
        "    epi = var.mean(dim=-1).mean(dim=-1) # [B]\n",
        "    return epi\n",
        "\n",
        "def combine_uq(aleatoric, epistemic, lam=0.5):\n",
        "    return lam * aleatoric + (1 - lam) * epistemic\n",
        "\n",
        "def make_weights_from_uq(uq_scores, alpha=1.0, scheme=\"exp\", min_w=0.1):\n",
        "    u = (uq_scores - uq_scores.min()) / (uq_scores.max() - uq_scores.min() + 1e-8)\n",
        "    if scheme == \"exp\":\n",
        "        w = torch.exp(-alpha * u)\n",
        "    elif scheme == \"linear\":\n",
        "        w = 1.0 - alpha * u\n",
        "    else:  # invert\n",
        "        w = 1.0 - u\n",
        "    return torch.clamp(w, min=min_w, max=1.0)\n",
        "\n",
        "def load_uq_weights(path):\n",
        "    df = pd.read_csv(path)\n",
        "    col_id = \"reaction_id\" if \"reaction_id\" in df.columns else df.columns[0]\n",
        "    col_uq = \"UQ_score\" if \"UQ_score\" in df.columns else df.columns[-1]\n",
        "    top = df[[col_id, col_uq]].dropna()\n",
        "    uq = (top[col_uq] - top[col_uq].min()) / (top[col_uq].max() - top[col_uq].min() + 1e-8)\n",
        "    weights = 1.0 - uq\n",
        "    return dict(zip(top[col_id].astype(str), weights.astype(float)))\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, stoi, opt, sch, uq_map, args):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for rids, src, tgt_in, tgt_y in loader:\n",
        "        src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "        logits = model(src, tgt_in)\n",
        "        per_ex_loss = ce_loss(logits, tgt_y, ignore_index=stoi[PAD], reduction=\"none\")\n",
        "\n",
        "        if uq_map:  # offline UQ\n",
        "            w = torch.tensor([uq_map.get(str(r), 1.0) for r in rids], device=DEVICE, dtype=per_ex_loss.dtype)\n",
        "        else:  # online UQ\n",
        "            alea = seq_entropy(logits)\n",
        "            epi  = mc_dropout_uq(model, src, tgt_in, n_samples=args.mc_samples) if args.mc_samples > 0 else torch.zeros_like(alea)\n",
        "            uq   = combine_uq(alea, epi, lam=args.lambda_mix)\n",
        "            w    = make_weights_from_uq(uq, alpha=args.alpha, scheme=args.weighting, min_w=args.min_w)\n",
        "\n",
        "        loss = (per_ex_loss * w).mean()\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step(); sch.step()\n",
        "\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, stoi):\n",
        "    model.eval()\n",
        "    tot, n = 0.0, 0\n",
        "    for _, src, tgt_in, tgt_y in loader:\n",
        "        src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = ce_loss(logits, tgt_y, ignore_index=stoi[PAD]).mean().item()\n",
        "        tot += loss; n += 1\n",
        "    return tot / max(1, n)\n",
        "\n",
        "def main(args):\n",
        "    stoi, itos = build_vocab([args.train, args.valid])\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    torch.save({\"stoi\": stoi, \"itos\": itos}, os.path.join(args.outdir, \"vocab.pt\"))\n",
        "\n",
        "    tr = RxDataset(args.train); va = RxDataset(args.valid)\n",
        "    C  = lambda b: collate(b, stoi)\n",
        "    dl_tr = DataLoader(tr, batch_size=args.bsz, shuffle=True,  collate_fn=C, num_workers=0)\n",
        "    dl_va = DataLoader(va, batch_size=args.bsz, shuffle=False, collate_fn=C, num_workers=0)\n",
        "\n",
        "    uq_map = load_uq_weights(args.uq_csv) if args.uq_csv else {}\n",
        "\n",
        "    model = MT(stoi).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1.0, betas=(0.9,0.98), weight_decay=0.01)\n",
        "    sch = noam_lr(opt, d_model=512, warmup=8000)\n",
        "\n",
        "    best, patience, bad = 1e9, args.patience, 0\n",
        "    for epoch in range(args.max_epochs):\n",
        "        tr_loss = train_one_epoch(model, dl_tr, stoi, opt, sch, uq_map, args)\n",
        "        va_loss = validate(model, dl_va, stoi)\n",
        "        print(f\"epoch {epoch} | train_loss {tr_loss:.4f} | val_loss {va_loss:.4f}\")\n",
        "        if va_loss < best:\n",
        "            best, bad = va_loss, 0\n",
        "            torch.save({\"model\": model.state_dict(), \"stoi\": stoi, \"itos\": itos},\n",
        "                       os.path.join(args.outdir, \"best.pt\"))\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience: break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--train\",  default=\"data/processed/train.txt\")\n",
        "    ap.add_argument(\"--valid\",  default=\"data/processed/valid.txt\")\n",
        "    ap.add_argument(\"--outdir\", default=\"runs/mt_uq_ce\")\n",
        "    ap.add_argument(\"--bsz\", type=int, default=64)\n",
        "    # UQ controls\n",
        "    ap.add_argument(\"--uq_csv\", default=\"\", help=\"Optional CSV with columns [reaction_id,UQ_score] for offline weights\")\n",
        "    ap.add_argument(\"--mc_samples\", type=int, default=0, help=\"If >0, do MC-dropout passes for epistemic UQ (online).\")\n",
        "    ap.add_argument(\"--lambda_mix\", type=float, default=0.5, help=\"Mix weight for aleatoric vs epistemic (0..1).\")\n",
        "    ap.add_argument(\"--alpha\", type=float, default=1.0, help=\"Strength of down-weighting uncertain samples.\")\n",
        "    ap.add_argument(\"--weighting\", choices=[\"exp\",\"linear\",\"invert\"], default=\"exp\")\n",
        "    ap.add_argument(\"--min_w\", type=float, default=0.1)\n",
        "    # Training control\n",
        "    ap.add_argument(\"--max_epochs\", type=int, default=8)\n",
        "    ap.add_argument(\"--patience\", type=int, default=5)\n",
        "    args = ap.parse_args(); main(args)\n",
        "    \"\"\"\n",
        "write_script(\"retroB/mt/train.py\", train_code)\n"
      ],
      "metadata": {
        "id": "d8qzhmGvWf-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Baseline + UQ + Label Smoothing Cross Entropy\n",
        "train_code = \"\"\"\n",
        "import argparse, os, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from tokenizer import build_vocab, tok_smiles, encode, PAD, BOS, EOS\n",
        "from model import MT, noam_lr\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# -------------------------\n",
        "# Dataset / Collate\n",
        "# -------------------------\n",
        "class RxDataset(Dataset):\n",
        "    def __init__(self, path): self.rows = [l.rstrip(\"\\\\n\").split(\"\\\\t\") for l in open(path)]\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self,i): rid, p, r = self.rows[i]; return rid, p, r\n",
        "\n",
        "def collate(batch, stoi, max_len=256):\n",
        "    rids, src, tgt = zip(*batch)\n",
        "    src = [[BOS]+tok_smiles(s)+[EOS] for s in src]\n",
        "    tgt = [[BOS]+tok_smiles(s)+[EOS] for s in tgt]\n",
        "    src_ids = [encode(s, stoi)[:max_len] for s in src]\n",
        "    tgt_ids = [encode(t, stoi)[:max_len] for t in tgt]\n",
        "    pad_id = stoi[PAD]\n",
        "    Ls = max(map(len, src_ids)); Lt = max(map(len, tgt_ids))\n",
        "    pad = lambda seq, L: seq + [pad_id]*(L-len(seq))\n",
        "    src_t = torch.tensor([pad(s,Ls) for s in src_ids]).long()\n",
        "    tgt_in = torch.tensor([pad(t[:-1],Lt-1) for t in tgt_ids]).long()\n",
        "    tgt_y  = torch.tensor([pad(t[1:], Lt-1) for t in tgt_ids]).long()\n",
        "    return rids, src_t, tgt_in, tgt_y\n",
        "\n",
        "# -------------------------\n",
        "# Label smoothing loss\n",
        "# -------------------------\n",
        "def label_smoothing_loss(logits, target, eps=0.1, ignore_index=0, reduction=\"none\"):\n",
        "    n_class = logits.size(-1)\n",
        "    logp = F.log_softmax(logits, -1)\n",
        "    with torch.no_grad():\n",
        "        true = torch.zeros_like(logp)\n",
        "        true.fill_(eps/(n_class-1))\n",
        "        true.scatter_(2, target.unsqueeze(-1), 1-eps)\n",
        "        true.masked_fill_(target.eq(ignore_index).unsqueeze(-1), 0)\n",
        "    loss = -(true*logp).sum(-1)\n",
        "    mask = (~target.eq(ignore_index)).float()\n",
        "    loss = loss * mask\n",
        "    if reduction == \"none\":\n",
        "        return loss.sum(-1) / mask.sum(-1)\n",
        "    else:\n",
        "        return loss.sum()/mask.sum()\n",
        "\n",
        "# -------------------------\n",
        "# Uncertainty helpers\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def seq_entropy(logits):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    log_probs = F.log_softmax(logits, dim=-1)\n",
        "    ent = -(probs * log_probs).sum(-1)\n",
        "    return ent.mean(-1)\n",
        "\n",
        "@torch.no_grad()\n",
        "def mc_dropout_uq(model, src, tgt_in, n_samples=6):\n",
        "    model.train()\n",
        "    preds = []\n",
        "    for _ in range(max(1, n_samples)):\n",
        "        logits = model(src, tgt_in)\n",
        "        preds.append(F.softmax(logits, dim=-1).detach())\n",
        "    stack = torch.stack(preds, dim=0)\n",
        "    var = stack.var(dim=0)\n",
        "    epi = var.mean(dim=-1).mean(dim=-1)\n",
        "    return epi\n",
        "\n",
        "def combine_uq(aleatoric, epistemic, lam=0.5):\n",
        "    return lam * aleatoric + (1 - lam) * epistemic\n",
        "\n",
        "def make_weights_from_uq(uq_scores, alpha=1.0, scheme=\"exp\", min_w=0.1):\n",
        "    u = (uq_scores - uq_scores.min()) / (uq_scores.max() - uq_scores.min() + 1e-8)\n",
        "    if scheme == \"exp\":\n",
        "        w = torch.exp(-alpha * u)\n",
        "    elif scheme == \"linear\":\n",
        "        w = 1.0 - alpha * u\n",
        "    else:\n",
        "        w = 1.0 - u\n",
        "    return torch.clamp(w, min=min_w, max=1.0)\n",
        "\n",
        "def load_uq_weights(path):\n",
        "    df = pd.read_csv(path)\n",
        "    col_id = \"reaction_id\" if \"reaction_id\" in df.columns else df.columns[0]\n",
        "    col_uq = \"UQ_score\" if \"UQ_score\" in df.columns else df.columns[-1]\n",
        "    top = df[[col_id, col_uq]].dropna()\n",
        "    uq = (top[col_uq] - top[col_uq].min()) / (top[col_uq].max() - top[col_uq].min() + 1e-8)\n",
        "    weights = 1.0 - uq\n",
        "    return dict(zip(top[col_id].astype(str), weights.astype(float)))\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, stoi, opt, sch, uq_map, args):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for rids, src, tgt_in, tgt_y in loader:\n",
        "        src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "        logits = model(src, tgt_in)\n",
        "        per_ex_loss = label_smoothing_loss(logits, tgt_y, ignore_index=stoi[PAD], reduction=\"none\")\n",
        "\n",
        "        if uq_map:\n",
        "            w = torch.tensor([uq_map.get(str(r), 1.0) for r in rids], device=DEVICE)\n",
        "        else:\n",
        "            alea = seq_entropy(logits)\n",
        "            epi  = mc_dropout_uq(model, src, tgt_in, n_samples=args.mc_samples) if args.mc_samples > 0 else torch.zeros_like(alea)\n",
        "            uq   = combine_uq(alea, epi, lam=args.lambda_mix)\n",
        "            w    = make_weights_from_uq(uq, alpha=args.alpha, scheme=args.weighting, min_w=args.min_w)\n",
        "\n",
        "        loss = (per_ex_loss * w).mean()\n",
        "        opt.zero_grad(); loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step(); sch.step()\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(loader))\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, stoi):\n",
        "    model.eval()\n",
        "    tot, n = 0.0, 0\n",
        "    for _, src, tgt_in, tgt_y in loader:\n",
        "        src, tgt_in, tgt_y = src.to(DEVICE), tgt_in.to(DEVICE), tgt_y.to(DEVICE)\n",
        "        logits = model(src, tgt_in)\n",
        "        loss = label_smoothing_loss(logits, tgt_y, ignore_index=stoi[PAD]).mean().item()\n",
        "        tot += loss; n += 1\n",
        "    return tot / max(1, n)\n",
        "\n",
        "def main(args):\n",
        "    stoi, itos = build_vocab([args.train, args.valid])\n",
        "    os.makedirs(args.outdir, exist_ok=True)\n",
        "    torch.save({\"stoi\": stoi, \"itos\": itos}, os.path.join(args.outdir, \"vocab.pt\"))\n",
        "\n",
        "    tr = RxDataset(args.train); va = RxDataset(args.valid)\n",
        "    C  = lambda b: collate(b, stoi)\n",
        "    dl_tr = DataLoader(tr, batch_size=args.bsz, shuffle=True, collate_fn=C)\n",
        "    dl_va = DataLoader(va, batch_size=args.bsz, shuffle=False, collate_fn=C)\n",
        "\n",
        "    uq_map = load_uq_weights(args.uq_csv) if args.uq_csv else {}\n",
        "\n",
        "    model = MT(stoi).to(DEVICE)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1.0, betas=(0.9,0.98), weight_decay=0.01)\n",
        "    sch = noam_lr(opt, d_model=512, warmup=8000)\n",
        "\n",
        "    best, patience, bad = 1e9, args.patience, 0\n",
        "    for epoch in range(args.max_epochs):\n",
        "        tr_loss = train_one_epoch(model, dl_tr, stoi, opt, sch, uq_map, args)\n",
        "        va_loss = validate(model, dl_va, stoi)\n",
        "        print(f\"epoch {epoch} | train_loss {tr_loss:.4f} | val_loss {va_loss:.4f}\")\n",
        "        if va_loss < best:\n",
        "            best, bad = va_loss, 0\n",
        "            torch.save({\"model\": model.state_dict(), \"stoi\": stoi, \"itos\": itos},\n",
        "                       os.path.join(args.outdir, \"best.pt\"))\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= patience: break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--train\",  default=\"data/processed/train.txt\")\n",
        "    ap.add_argument(\"--valid\",  default=\"data/processed/valid.txt\")\n",
        "    ap.add_argument(\"--outdir\", default=\"runs/mt\")\n",
        "    ap.add_argument(\"--bsz\", type=int, default=64)\n",
        "    ap.add_argument(\"--uq_csv\", default=\"\")\n",
        "    ap.add_argument(\"--mc_samples\", type=int, default=0)\n",
        "    ap.add_argument(\"--lambda_mix\", type=float, default=0.5)\n",
        "    ap.add_argument(\"--alpha\", type=float, default=1.0)\n",
        "    ap.add_argument(\"--weighting\", choices=[\"exp\",\"linear\",\"invert\"], default=\"exp\")\n",
        "    ap.add_argument(\"--min_w\", type=float, default=0.1)\n",
        "    ap.add_argument(\"--max_epochs\", type=int, default=8)\n",
        "    ap.add_argument(\"--patience\", type=int, default=5)\n",
        "    args = ap.parse_args(); main(args)\n",
        "    \"\"\"\n",
        "write_script(\"retroB/mt/train.py\", train_code)\n"
      ],
      "metadata": {
        "id": "_odg3lOBWc8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "infer_code = \"\"\"\n",
        "import argparse, torch, numpy as np, pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from tokenizer import PAD, BOS, EOS, decode\n",
        "from model import MT\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ------------------------------\n",
        "# Combine Aleatoric + Epistemic\n",
        "# ------------------------------\n",
        "def compute_combined_uq(aleatoric, epistemic, method=\"weighted_sum\", alea_weight=0.5, epis_weight=0.5):\n",
        "    if method == \"weighted_sum\":\n",
        "        combined = alea_weight * aleatoric + epis_weight * epistemic\n",
        "    elif method == \"geometric_mean\":\n",
        "        total = alea_weight + epis_weight\n",
        "        w_alea = alea_weight / total\n",
        "        w_epis = epis_weight / total\n",
        "        combined = np.power(aleatoric, w_alea) * np.power(epistemic, w_epis)\n",
        "    elif method == \"rss\":\n",
        "        combined = np.sqrt(np.square(aleatoric) + np.square(epistemic))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported combination method: {method}\")\n",
        "    return combined\n",
        "\n",
        "# ------------------------------\n",
        "# Token entropy = Aleatoric proxy\n",
        "# ------------------------------\n",
        "def token_entropy(step_logits):  # [T,V]\n",
        "    p = F.softmax(step_logits, dim=-1)\n",
        "    return -(p * (p.clamp_min(1e-12)).log()).sum(-1)  # [T]\n",
        "\n",
        "# ------------------------------\n",
        "# Beam search decoder\n",
        "# ------------------------------\n",
        "@torch.no_grad()\n",
        "def beam_search(model, src, beam=10, max_len=256, pad_id=0, bos_id=1, eos_id=2):\n",
        "    beams = [([bos_id], 0.0, [])]\n",
        "    finished = []\n",
        "    for _ in range(max_len):\n",
        "        new = []\n",
        "        for tokens, lp, slogits in beams:\n",
        "            inp = torch.tensor([tokens], device=src.device)\n",
        "            logits = model(src, inp)[:,-1,:]                 # [1,V]\n",
        "            logp = F.log_softmax(logits, -1).squeeze(0)      # [V]\n",
        "            vals, idxs = torch.topk(logp, k=beam)\n",
        "            for addlp, tok in zip(vals.tolist(), idxs.tolist()):\n",
        "                ntoks = tokens+[tok]; nlp = lp+addlp\n",
        "                nslog = slogits+[logits.squeeze(0)]\n",
        "                if tok==eos_id: finished.append((ntoks, nlp, nslog))\n",
        "                else: new.append((ntoks, nlp, nslog))\n",
        "        beams = sorted(new, key=lambda x: x[1], reverse=True)[:beam]\n",
        "        if len(finished)>=beam: break\n",
        "    if not finished: finished = beams\n",
        "    finished = sorted(finished, key=lambda x: x[1], reverse=True)[:beam]\n",
        "    return finished\n",
        "\n",
        "# ------------------------------\n",
        "# Epistemic proxy via MC Dropout\n",
        "# ------------------------------\n",
        "def mc_dropout_var(model, src, bos_id, eos_id, passes=5, beam=1):\n",
        "    model.train()  # keep dropout active\n",
        "    vals = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(passes):\n",
        "            beams = beam_search(model, src, beam=beam, bos_id=bos_id, eos_id=eos_id)\n",
        "            vals.append(beams[0][1])   # take seq logprob of top-1\n",
        "    model.eval()\n",
        "    return np.var(vals)\n",
        "\n",
        "# ------------------------------\n",
        "# Main Inference\n",
        "# ------------------------------\n",
        "def main(args):\n",
        "    ckpt = torch.load(args.ckpt, map_location=DEVICE)\n",
        "    stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
        "    model = MT(stoi).to(DEVICE); model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "    pad_id, bos_id, eos_id = stoi[PAD], stoi[BOS], stoi[EOS]\n",
        "\n",
        "    def enc(prod_str):\n",
        "        ids = [bos_id] + [stoi[c] for c in list(prod_str)] + [eos_id]\n",
        "        return torch.tensor([ids], device=DEVICE)\n",
        "\n",
        "    rows = [l.rstrip(\"\\\\n\").split(\"\\t\") for l in open(args.test)]\n",
        "    out = []\n",
        "    for rid, prod, gold in rows:\n",
        "        src = enc(prod)\n",
        "\n",
        "        # epistemic once per product\n",
        "        epistemic = mc_dropout_var(model, src, bos_id, eos_id, passes=5)\n",
        "\n",
        "        beams = beam_search(model, src, beam=args.beam, pad_id=pad_id, bos_id=bos_id, eos_id=eos_id)\n",
        "        scores = np.array([b[1] for b in beams])\n",
        "        probs = np.exp(scores - scores.max()); probs = probs / probs.sum()\n",
        "\n",
        "        for rank, (tokens, logp, slogits) in enumerate(beams, start=1):\n",
        "            step_logits = torch.stack(slogits)             # [T,V]\n",
        "            ents = token_entropy(step_logits).cpu().numpy()\n",
        "            pred = decode(tokens[1:-1], itos)              # strip BOS/EOS\n",
        "\n",
        "            # Aleatoric proxy = mean entropy\n",
        "            aleatoric = float(ents.mean())\n",
        "\n",
        "            # Combined UQ using chosen method\n",
        "            UQ_score = compute_combined_uq(\n",
        "                aleatoric, float(epistemic),\n",
        "                method=args.uq_method,\n",
        "                alea_weight=args.alea_weight,\n",
        "                epis_weight=args.epis_weight\n",
        "            )\n",
        "\n",
        "            row = {\n",
        "                \"reaction_id\": rid,\n",
        "                \"product\": prod,\n",
        "                \"rank\": rank,\n",
        "                \"pred_reactants\": pred,\n",
        "                \"seq_logprob\": float(logp),\n",
        "                \"aleatoric_entropy\": aleatoric,\n",
        "                \"epistemic_var\": float(epistemic),\n",
        "                \"UQ_score\": float(UQ_score),\n",
        "                \"confidence\": float(probs[rank-1]),\n",
        "                \"gold_reactants\": gold\n",
        "            }\n",
        "\n",
        "            if rank == 1:  # save logits only for top beam\n",
        "                row[\"logits\"] = step_logits.cpu().numpy().tolist()\n",
        "\n",
        "            out.append(row)\n",
        "\n",
        "    pd.DataFrame(out).to_json(args.outcsv.replace(\".csv\", \"_with_logits.json\"), orient=\"records\")\n",
        "    pd.DataFrame(out).to_csv(args.outcsv, index=False)\n",
        "\n",
        "# ------------------------------\n",
        "# CLI\n",
        "# ------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--ckpt\",  default=\"runs/mt/best.pt\")\n",
        "    ap.add_argument(\"--test\",  default=\"data/processed/test.txt\")\n",
        "    ap.add_argument(\"--beam\",  type=int, default=10)\n",
        "    ap.add_argument(\"--outcsv\", default=\"out/mt_preds_with_uq.csv\")\n",
        "    ap.add_argument(\"--uq_method\", type=str, default=\"weighted_sum\",\n",
        "                    choices=[\"weighted_sum\", \"geometric_mean\", \"rss\"],\n",
        "                    help=\"Method for combining aleatoric & epistemic UQ\")\n",
        "    ap.add_argument(\"--alea_weight\", type=float, default=0.5,\n",
        "                    help=\"Weight for aleatoric (for weighted_sum/geometric_mean)\")\n",
        "    ap.add_argument(\"--epis_weight\", type=float, default=0.5,\n",
        "                    help=\"Weight for epistemic (for weighted_sum/geometric_mean)\")\n",
        "    args = ap.parse_args(); main(args)\n",
        "\"\"\"\n",
        "write_script(\"retroB/mt/infer.py\", infer_code)\n"
      ],
      "metadata": {
        "id": "SHYjAy20WdGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_code = \"\"\"\n",
        "import argparse, pandas as pd, numpy as np, json, torch, os\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "\n",
        "# --------------------\n",
        "# Suppress RDKit warnings (helper)\n",
        "# --------------------\n",
        "RDLogger.DisableLog(\"rdApp.*\")\n",
        "\n",
        "# --------------------\n",
        "# Canonicalize SMILES (helper)\n",
        "# --------------------\n",
        "def canonicalize(smiles):\n",
        "    \\\"\\\"\\\"Convert SMILES to canonical form, return None if invalid.\\\"\\\"\\\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        return Chem.MolToSmiles(mol, canonical=True)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# --------------------\n",
        "# Top-k Accuracy (metric)\n",
        "# --------------------\n",
        "def topk(df):\n",
        "    def acc_at(k):\n",
        "        d = df[df[\"rank\"] <= k] \\\\\n",
        "             .assign(correct=lambda x: (\n",
        "                 x[\"pred_reactants\"].apply(canonicalize) ==\n",
        "                 x[\"gold_reactants\"].apply(canonicalize)\n",
        "             ).astype(int)) \\\\\n",
        "             .groupby(\"reaction_id\")[\"correct\"].max()\n",
        "        return d.mean()\n",
        "    return {f\"top{k}\": float(acc_at(k)) for k in [1,5,10]}\n",
        "\n",
        "# --------------------\n",
        "# Expected Calibration Error (metric)\n",
        "# --------------------\n",
        "def ece(conf, corr, n_bins=15):\n",
        "    bins = np.linspace(0.,1.,n_bins+1); e=0.0\n",
        "    for i in range(n_bins):\n",
        "        m = (conf > bins[i]) & (conf <= bins[i+1])\n",
        "        if m.any(): e += m.mean() * abs(corr[m].mean() - conf[m].mean())\n",
        "    return float(e)\n",
        "\n",
        "# --------------------\n",
        "# Brier Score (metric)\n",
        "# --------------------\n",
        "def brier(conf, corr):\n",
        "    return float(((conf - corr.astype(float))**2).mean())\n",
        "\n",
        "# --------------------\n",
        "# Reliability Plot (helper)\n",
        "# --------------------\n",
        "def reliability_plot(conf, corr, n_bins=10, title=\"Reliability Diagram\", outpath=None):\n",
        "    bins = np.linspace(0, 1, n_bins+1)\n",
        "    binids = np.digitize(conf, bins) - 1\n",
        "    accuracies, confidences = [], []\n",
        "\n",
        "    for b in range(n_bins):\n",
        "        idx = binids == b\n",
        "        if np.any(idx):\n",
        "            accuracies.append(corr[idx].mean())\n",
        "            confidences.append(conf[idx].mean())\n",
        "        else:\n",
        "            accuracies.append(0.0)\n",
        "            confidences.append(0.0)\n",
        "\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.plot([0,1],[0,1],\"--\",color=\"gray\")\n",
        "    plt.bar(bins[:-1], accuracies, width=1/n_bins, alpha=0.6, label=\"Accuracy\")\n",
        "    plt.plot(bins[:-1]+0.05, confidences, \"o-\", color=\"red\", label=\"Confidence\")\n",
        "    plt.xlabel(\"Confidence\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    if outpath:\n",
        "        os.makedirs(os.path.dirname(outpath), exist_ok=True)\n",
        "        plt.savefig(outpath, bbox_inches=\"tight\")\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()\n",
        "\n",
        "# --------------------\n",
        "# Temperature Scaling (calibration)\n",
        "# --------------------\n",
        "def set_temperature(logits, labels):\n",
        "    nll = torch.nn.CrossEntropyLoss()\n",
        "    T = torch.nn.Parameter(torch.ones(1) * 1.5)\n",
        "    optimizer = torch.optim.LBFGS([T], lr=0.01, max_iter=50)\n",
        "\n",
        "    def eval():\n",
        "        optimizer.zero_grad()\n",
        "        loss = nll(logits / T, labels)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(eval)\n",
        "    return T.item()\n",
        "\n",
        "# --------------------\n",
        "# Retro-BLEU (plausibility metric)\n",
        "# --------------------\n",
        "def retro_bleu(preds, refs, n_gram=4):\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    scores = []\n",
        "    for p, r in zip(preds, refs):\n",
        "        if p is None or r is None:\n",
        "            continue\n",
        "        p_tokens, r_tokens = list(p), list(r)\n",
        "        try:\n",
        "            score = sentence_bleu([r_tokens], p_tokens,\n",
        "                                  weights=tuple([1/n_gram]*n_gram),\n",
        "                                  smoothing_function=smoothie)\n",
        "        except ZeroDivisionError:\n",
        "            score = 0.0\n",
        "        scores.append(score)\n",
        "    return float(np.mean(scores)) if scores else 0.0\n",
        "\n",
        "# --------------------\n",
        "# Plausibility Score (valid SMILES fraction)\n",
        "# --------------------\n",
        "def plausibility_score(smiles_list):\n",
        "    valid = 0\n",
        "    total = 0\n",
        "    for s in smiles_list:\n",
        "        if s is None:\n",
        "            continue\n",
        "        total += 1\n",
        "        mol = Chem.MolFromSmiles(s)\n",
        "        if mol is not None:\n",
        "            valid += 1\n",
        "    return valid / total if total > 0 else 0.0\n",
        "\n",
        "# --------------------\n",
        "# Main Script\n",
        "# --------------------\n",
        "if __name__ == \"__main__\":\n",
        "    ap = argparse.ArgumentParser()\n",
        "    ap.add_argument(\"--csv\", default=\"out/mt_preds_with_uq.csv\")\n",
        "    ap.add_argument(\"--json_logits\", default=\"out/mt_preds_with_uq_with_logits.json\")\n",
        "    ap.add_argument(\"--out\", default=\"out/metrics.json\")\n",
        "    args = ap.parse_args()\n",
        "\n",
        "    # Load data (helper)\n",
        "    df = pd.read_csv(args.csv)\n",
        "    with open(args.json_logits, \"r\") as f:\n",
        "        data_logits = json.load(f)\n",
        "\n",
        "    res = topk(df)\n",
        "\n",
        "    # Top-1 correctness (metric)\n",
        "    top1 = df[df[\"rank\"] == 1].copy()\n",
        "    top1[\"correct\"] = (\n",
        "        top1[\"pred_reactants\"].apply(canonicalize) ==\n",
        "        top1[\"gold_reactants\"].apply(canonicalize)\n",
        "    ).astype(int)\n",
        "\n",
        "    # Before calibration (metrics + plot)\n",
        "    conf = top1[\"confidence\"].to_numpy()\n",
        "    corr = top1[\"correct\"].to_numpy()\n",
        "    res[\"ece_top1_before\"]   = ece(conf, corr)\n",
        "    res[\"brier_top1_before\"] = brier(conf, corr)\n",
        "    reliability_plot(conf, corr, n_bins=10,\n",
        "                     title=\"Reliability (Before Calibration)\",\n",
        "                     outpath=\"out/reliability_before.png\")\n",
        "\n",
        "    # Prepare logits + labels (helper)\n",
        "    logits_list, labels = [], []\n",
        "    for row in data_logits:\n",
        "        if row[\"rank\"] == 1 and \"logits\" in row:\n",
        "            step_logits = np.array(row[\"logits\"])  # [T,V]\n",
        "            final_logits = step_logits[-1]\n",
        "            logits_list.append(final_logits)\n",
        "            labels.append(1 if canonicalize(row[\"pred_reactants\"]) ==\n",
        "                             canonicalize(row[\"gold_reactants\"]) else 0)\n",
        "\n",
        "    logits_tensor = torch.tensor(np.stack(logits_list), dtype=torch.float32)\n",
        "    labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Temperature scaling (calibration)\n",
        "    T = set_temperature(logits_tensor, labels_tensor)\n",
        "    scaled_probs = torch.softmax(logits_tensor / T, dim=1).detach().numpy()\n",
        "    calibrated_conf = scaled_probs.max(axis=1)\n",
        "\n",
        "    # After calibration (metrics + plot)\n",
        "    res[\"ece_top1_after\"]   = ece(calibrated_conf, np.array(labels))\n",
        "    res[\"brier_top1_after\"] = brier(calibrated_conf, np.array(labels))\n",
        "    reliability_plot(calibrated_conf, np.array(labels), n_bins=10,\n",
        "                     title=f\"Reliability (After Calibration, T={T:.2f})\",\n",
        "                     outpath=\"out/reliability_after.png\")\n",
        "\n",
        "    # Plausibility metrics (metrics)\n",
        "    preds = [canonicalize(s) for s in top1[\"pred_reactants\"].tolist()]\n",
        "    refs  = [canonicalize(s) for s in top1[\"gold_reactants\"].tolist()]\n",
        "    res[\"retro_bleu_top1\"]   = retro_bleu(preds, refs)\n",
        "    res[\"plausibility_top1\"] = plausibility_score(preds)\n",
        "\n",
        "    # Save metrics (helper)\n",
        "    os.makedirs(os.path.dirname(args.out), exist_ok=True)\n",
        "    with open(args.out, \"w\") as f: json.dump(res, f, indent=2)\n",
        "    print(json.dumps(res, indent=2))\n",
        "\"\"\"\n",
        "\n",
        "write_script(\"retroB/mt/metrics.py\", metrics_code)"
      ],
      "metadata": {
        "id": "njCoXM-OWdTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "_43jcpIqWvDY",
        "outputId": "06ad3904-cfe1-42a4-e9d9-74c5ce2dba67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3f7e2254-52ab-4c39-8b55-bc32d81f1650\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3f7e2254-52ab-4c39-8b55-bc32d81f1650\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving raw_test.csv to raw_test.csv\n",
            "Saving raw_train.csv to raw_train.csv\n",
            "Saving raw_val.csv to raw_val.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.move(\"raw_train.csv\", \"retroB/data/raw_train.csv\")\n",
        "shutil.move(\"raw_val.csv\", \"retroB/data/raw_val.csv\")\n",
        "shutil.move(\"raw_test.csv\", \"retroB/data/raw_test.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "q-NdXSmWWwX1",
        "outputId": "e571e268-e9d9-4235-f51c-96b9cdb07aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'retroB/data/raw_test.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 retroB/data/prepare_uspto.py \\\n",
        "    --csv retroB/data/raw_train.csv \\\n",
        "    --outdir retroB/data/processed\n",
        "\n",
        "!python3 retroB/data/prepare_uspto.py \\\n",
        "    --csv retroB/data/raw_val.csv \\\n",
        "    --outdir retroB/data/processed\n",
        "\n",
        "!python3 retroB/data/prepare_uspto.py \\\n",
        "    --csv retroB/data/raw_test.csv \\\n",
        "    --outdir retroB/data/processed"
      ],
      "metadata": {
        "id": "KGyu8_7oWwN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 190 retroB/data/processed/train.txt > retroB/data/processed/train_debug.txt"
      ],
      "metadata": {
        "id": "LsSa05tPFlpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 190 retroB/data/processed/test.txt > retroB/data/processed/test_debug.txt"
      ],
      "metadata": {
        "id": "lqD3m3WwGpIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 190 retroB/data/processed/valid.txt > retroB/data/processed/valid_debug.txt"
      ],
      "metadata": {
        "id": "PHJFbZZaGp8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"retroB/data/processed/train.txt\") as f:\n",
        "    for i in range(5):\n",
        "        print(f.readline().strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asJ6vN6jd9Ci",
        "outputId": "7aebf5ad-aca6-446c-e374-fb6ec2cff811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "849\tCOc1ncc(-c2ccc3ncc4nnc(C)n4c3c2)cc1N\tCOc1ncc(B2OC(C)(C)C(C)(C)O2)cc1N.Cc1nnc2cnc3ccc(Br)cc3n12\n",
            "4185\tCCCCCCCCCCCCCCCCCCCCCCOc1ccc(CO)c(OCCCCCCCCCCCCCCCCCCCCCC)c1\tCCCCCCCCCCCCCCCCCCCCCCOc1ccc(C=O)c(OCCCCCCCCCCCCCCCCCCCCCC)c1\n",
            "4373\tCCCn1c(COCC)nc2c(N)nc3cc(OC4CCN(C(=O)NC(C)C)CC4)ccc3c21\tCC(C)N=C=O.CCCn1c(COCC)nc2c(N)nc3cc(OC4CCNCC4)ccc3c21\n",
            "4715\tOc1ccc(-c2nnc(CSCCOc3ccccc3)o2)cc1\tO=C(CSCCOc1ccccc1)NNC(=O)c1ccc(O)cc1\n",
            "654\tCC(C)(C)OC(=O)N1CCC(C(=O)Nc2cc(OCc3ccccc3)ccc2Br)CC1\tCC(C)(C)OC(=O)N1CCC(C(=O)O)CC1.Nc1cc(OCc2ccccc2)ccc1Br\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "\n",
        "def canonicalize(smi):\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smi)\n",
        "        if mol: return Chem.MolToSmiles(mol, canonical=True)\n",
        "    except:\n",
        "        return None\n",
        "    return None\n",
        "\n",
        "print(canonicalize(\"OCC\"))   # should print \"CCO\"\n",
        "print(canonicalize(\"CCO\"))   # should also print \"CCO\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XvHTzFikAAJ",
        "outputId": "2a64cb9a-181b-40b3-ac13-bed342e780f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CCO\n",
            "CCO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "raw = pd.read_csv(\"retroB/data/raw_train.csv\")\n",
        "print(raw.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vUDPWeDeAJy",
        "outputId": "a3259abd-8ee8-45b6-9dfe-4fbfe7ceb8de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                id class                      reactants>reagents>production\n",
            "0       US05849732   UNK  O=C(OCc1ccccc1)[NH:1][CH2:2][CH2:3][CH2:4][CH2...\n",
            "1  US20120114765A1   UNK  O[C:1](=[O:2])[c:3]1[cH:4][c:5]([N+:6](=[O:7])...\n",
            "2     US08003648B2   UNK  O=[CH:1][c:2]1[cH:3][cH:4][c:5](-[c:6]2[n:7][c...\n",
            "3     US09045475B2   UNK  O=[C:1]([CH2:2][F:3])[CH2:4][F:5].[CH3:6][C:7]...\n",
            "4     US08188098B2   UNK  Cl[C:1](=[O:2])[O:3][CH:4]1[CH2:5][CH2:6][CH2:...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"./retroB/mt\")\n",
        "\n",
        "from tokenizer import decode\n",
        "from model import MT\n",
        "import torch\n",
        "\n",
        "ckpt = torch.load(\"retroB/runs/mt_standardce/best.pt\", map_location=\"cpu\")\n",
        "stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
        "model = MT(stoi); model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "# Inspect one sample from train.txt\n",
        "with open(\"retroB/data/processed/train.txt\") as f:\n",
        "    rid, product, gold_reactants = f.readline().strip().split(\"\\t\")\n",
        "\n",
        "print(\"Product:\", product)\n",
        "print(\"Gold reactants:\", gold_reactants)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqrBFtO0ek-G",
        "outputId": "b6cc99cb-a758-4ac5-a467-9442083b4c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product: COc1ncc(-c2ccc3ncc4nnc(C)n4c3c2)cc1N\n",
            "Gold reactants: COc1ncc(B2OC(C)(C)C(C)(C)O2)cc1N.Cc1nnc2cnc3ccc(Br)cc3n12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline with standard cross-entropy\n",
        "!python3 retroB/mt/train.py \\\n",
        "  --train retroB/data/processed/train_debug.txt \\\n",
        "  --valid retroB/data/processed/valid_debug.txt \\\n",
        "  --outdir retroB/runs/mt_standardce \\\n",
        "  --bsz 8 \\\n",
        "  --max_epochs 5 \\\n",
        "  --patience 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUKS8A-KW4eh",
        "outputId": "9abed0c6-7885-434c-fe0e-33df4c93d5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/retroB/mt/train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tokenizer import decode, PAD, BOS, EOS\n",
        "from model import MT\n",
        "\n",
        "ckpt = torch.load(\"retroB/runs/mt_standardce/best.pt\", map_location=\"cpu\")\n",
        "stoi, itos = ckpt[\"stoi\"], ckpt[\"itos\"]\n",
        "model = MT(stoi); model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
        "\n",
        "def enc(prod_str):\n",
        "    ids = [stoi[BOS]] + [stoi[c] for c in prod_str] + [stoi[EOS]]\n",
        "    return torch.tensor([ids])\n",
        "\n",
        "product = \"CCO\"  # ethanol\n",
        "src = enc(product)\n",
        "with torch.no_grad():\n",
        "    logits = model(src, src[:,:-1])  # greedy test\n",
        "    pred_ids = logits.argmax(-1).squeeze().tolist()\n",
        "    pred = decode(pred_ids, itos)\n",
        "\n",
        "print(\"Predicted:\", pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fZ5DrQKlIx-",
        "outputId": "b2f8b2d1-d41d-4ddf-929f-47ceb49bdc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 3333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline with labelsmoothing cross-entropy\n",
        "!python3 retroB/mt/train.py \\\n",
        "  --train retroB/data/processed/train.txt \\\n",
        "  --valid retroB/data/processed/valid.txt \\\n",
        "  --outdir retroB/runs/mt_labelsmce \\\n",
        "  --bsz 64 \\\n",
        "  --max_epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shSmiWRnb-zz",
        "outputId": "3dd28ca0-e297-44de-c286-9a4b9db08cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 | val_loss 1.2139\n",
            "epoch 1 | val_loss 0.9564\n",
            "epoch 2 | val_loss 0.8720\n",
            "epoch 3 | val_loss 0.8320\n",
            "epoch 4 | val_loss 0.7991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline with UQ and standard cross-entropy (online UQ)\n",
        "!python3 retroB/mt/train.py \\\n",
        "  --train retroB/data/processed/train_debug.txt \\\n",
        "  --valid retroB/data/processed/valid_debug.txt \\\n",
        "  --outdir retroB/runs/mt_uq_online_ce \\\n",
        "  --bsz 64 \\\n",
        "  --max_epochs 5 \\\n",
        "  --mc_samples 6 \\\n",
        "  --lambda_mix 0.5 \\\n",
        "  --alpha 1.0 \\\n",
        "  --weighting exp \\\n",
        "  --min_w 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNOt_xBRcF6c",
        "outputId": "fe0c1b36-a26b-45e0-e83c-2819782242f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 | train_loss 2.5840 | val_loss 4.0902\n",
            "epoch 1 | train_loss 2.5009 | val_loss 4.0549\n",
            "epoch 2 | train_loss 2.6223 | val_loss 3.9992\n",
            "epoch 3 | train_loss 2.5906 | val_loss 3.9243\n",
            "epoch 4 | train_loss 2.4536 | val_loss 3.8320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline with UQ and labelsmoothing cross-entropy\n",
        "!python3 retroB/mt/train.py \\\n",
        "  --train retroB/data/processed/train.txt \\\n",
        "  --valid retroB/data/processed/valid.txt \\\n",
        "  --outdir retroB/runs/mt_uq_labelsmce \\\n",
        "  --bsz 64 \\\n",
        "  --max_epochs 5 \\\n",
        "  --uq_csv retroB/data/processed/train_uq_scores.csv"
      ],
      "metadata": {
        "id": "a6LoaqyMcTIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and Standard Cross-Entropy\n",
        "!python3 retroB/mt/infer.py \\\n",
        "    --ckpt retroB/runs/mt_uq_online_ce/best.pt \\\n",
        "    --test retroB/data/processed/test_debug.txt \\\n",
        "    --beam 1 \\\n",
        "    --uq_method geometric_mean \\\n",
        "    --alea_weight 0.7 \\\n",
        "    --epis_weight 0.3 \\\n",
        "    --outcsv retroB/out/mt_uq_preds.csv"
      ],
      "metadata": {
        "id": "7ux8owD1W3js",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96bee6bb-1592-47bb-d81b-0ac815064ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/retroB/mt/infer.py\", line 150, in <module>\n",
            "    args = ap.parse_args(); main(args)\n",
            "                            ^^^^^^^^^^\n",
            "  File \"/content/retroB/mt/infer.py\", line 75, in main\n",
            "    ckpt = torch.load(args.ckpt, map_location=DEVICE)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'retroB/runs/mt_uq_online_ce/best.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and Label Smoothing Cross-Entropy\n",
        "!python3 retroB/mt/infer.py \\\n",
        "  --ckpt retroB/runs/mt_labelsmce/best.pt \\\n",
        "  --test retroB/data/processed/test_debug.txt \\\n",
        "  --beam 1 \\\n",
        "  --outcsv retroB/out/mt_labelsmce_preds.csv"
      ],
      "metadata": {
        "id": "Wxy7UlJ1c3EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and UQ and Standard Cross-Entropy\n",
        "!python3 retroB/mt/infer.py \\\n",
        "  --ckpt retroB/runs/mt_uq_online_ce/best.pt \\\n",
        "  --test retroB/data/processed/test_debug.txt \\\n",
        "  --beam 1 \\\n",
        "  --outcsv retroB/out/mt_uq_ce_preds.csv"
      ],
      "metadata": {
        "id": "m9IPrTx5c8Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and UQ and Label Smoothing Cross-Entropy\n",
        "!python3 retroB/mt/infer.py \\\n",
        "  --ckpt retroB/runs/mt_uq_labelsmce/best.pt \\\n",
        "  --test retroB/data/processed/test_debug.txt \\\n",
        "  --beam 1 \\\n",
        "  --outcsv retroB/out/mt_uq_labelsmce_preds.csv"
      ],
      "metadata": {
        "id": "qzhzwfoVdC8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##METRICS PART BELOW SECTIONED IN 4 PARTS"
      ],
      "metadata": {
        "id": "IuZtf2Kjdz-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and Standard Cross-Entropy\n",
        "!python3 retroB/mt/metrics.py \\\n",
        "  --csv retroB/out/mt_standardce_preds.csv \\\n",
        "  --json_logits retroB/out/mt_standardce_preds_with_logits.json \\\n",
        "  --out retroB/out/mt_standardce_metrics.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOI24XqwW_0P",
        "outputId": "1a4b6885-ebb7-496f-e7b5-53b44a9c6dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  loss = float(closure())\n",
            "{\n",
            "  \"top1\": 0.0,\n",
            "  \"top5\": 0.0,\n",
            "  \"top10\": 0.0,\n",
            "  \"ece_top1_before\": 1.0,\n",
            "  \"brier_top1_before\": 1.0,\n",
            "  \"ece_top1_after\": 0.8438736224174499,\n",
            "  \"brier_top1_after\": 0.750165443642741,\n",
            "  \"retro_bleu_top1\": 0.7864217469357003,\n",
            "  \"plausibility_top1\": 1.0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and Labeling Smoothing Cross-Entropy\n",
        "!python3 retroB/mt/metrics.py \\\n",
        "  --csv retroB/out/mt_labelsmce_preds.csv \\\n",
        "  --json_logits retroB/out/mt_labelsmce_preds_with_logits.json \\\n",
        "  --out retroB/out/mt_labelsmce_metrics.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwDClzC3dRm3",
        "outputId": "628ececa-4ca3-49d2-cb17-e3ede54f1536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  loss = float(closure())\n",
            "{\n",
            "  \"top1\": 0.0,\n",
            "  \"top5\": 0.0,\n",
            "  \"top10\": 0.0,\n",
            "  \"ece_top1_before\": 1.0,\n",
            "  \"brier_top1_before\": 1.0,\n",
            "  \"ece_top1_after\": 0.40727545738220217,\n",
            "  \"brier_top1_after\": 0.1715270954949243,\n",
            "  \"retro_bleu_top1\": 0.6462103208837113,\n",
            "  \"plausibility_top1\": 1.0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and UQ and Standard Cross-Entropy\n",
        "!python3 retroB/mt/metrics.py \\\n",
        "  --csv retroB/out/mt_uq_ce_preds.csv \\\n",
        "  --json_logits retroB/out/mt_uq_ce_preds_with_logits.json \\\n",
        "  --out retroB/out/mt_uq_ce_metrics.json"
      ],
      "metadata": {
        "id": "I3pb4ON1dYlu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f71702b-712b-4b14-db70-487ba203b60d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/optim/lbfgs.py:457: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
            "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
            "  loss = float(closure())\n",
            "{\n",
            "  \"top1\": 0.0,\n",
            "  \"top5\": 0.0,\n",
            "  \"top10\": 0.0,\n",
            "  \"ece_top1_before\": 1.0,\n",
            "  \"brier_top1_before\": 1.0,\n",
            "  \"ece_top1_after\": 0.047933172434568405,\n",
            "  \"brier_top1_after\": 0.0023018148618795965,\n",
            "  \"retro_bleu_top1\": 0.00023626519801865363,\n",
            "  \"plausibility_top1\": 1.0\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Baseline and UQ and Label Smoothing Cross-Entropy\n",
        "!python3 retroB/mt/metrics.py \\\n",
        "  --csv retroB/out/mt_uq_labelsmce_preds.csv \\\n",
        "  --json_logits retroB/out/mt_uq_labelsmce_preds.json \\\n",
        "  --out retroB/out/mt_uq_labelsmce_metrics.json"
      ],
      "metadata": {
        "id": "Jw8ZFpfQdew0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## mt_pred_with_uq contains rows pertaining to one test example (set of reactants) and columns include true product, model's top prediction, two other likely alternatives, score 1 to 3, entropy(low is better), and overall log-likelihood\n",
        "## metrics.json contains aggregate performance metrics computed over the entire test set, based on the predictions in mt_preds_with_uq.csv. Top1 accuracy, top2-3 as well as top5-10, average log likelihood, average entropy, n_test examples evaluated, calibration error (predicted confidence matches the actual accuracy of the model)\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/retroB/out/mt_preds_with_uq_debug.csv')\n",
        "files.download('/content/retroB/out/metrics.json')"
      ],
      "metadata": {
        "id": "MCwZHvHjXCjU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}